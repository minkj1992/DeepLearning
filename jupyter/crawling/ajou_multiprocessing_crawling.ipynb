{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.6699912548065186 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# --- 2.43959641456604 seconds ---\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# \n",
    "\n",
    "# 페이지네이션\n",
    "def spider(max_pages=1):\n",
    "    page = 0\n",
    "    content_num = 10\n",
    "    data = {}\n",
    "    while page < max_pages:\n",
    "#       우리학교 url은 offset 번호를 바꾸어주면 page가 바뀐다.\n",
    "        url = 'http://www.ajou.ac.kr/main/ajou/notice.jsp?mode=list&board_no=33&pager.offset='+str(page*content_num)\n",
    "        req = requests.get(url)\n",
    "        html = req.text \n",
    "#         html.parser vs lxml\n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "        \n",
    "        for row in soup.find_all(\"tr\")[1:]:\n",
    "            tmp = row.find(\"a\")\n",
    "#             a링크에 붙여진 link를 붙여준다.\n",
    "            href = \"http://www.ajou.ac.kr/main/ajou/notice.jsp\" + tmp['href'] \n",
    "            data[tmp.text] = href\n",
    "#             print(data)\n",
    "        page += 1\n",
    "    \n",
    "    return data\n",
    "\n",
    "        \n",
    "def get_single_article(item_url):\n",
    "    req = requests.get(item_url)\n",
    "    html = req.text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "#     print(\"Inside_contents:\" + str(soup.find('div', {\"id\": \"article_text\"})))\n",
    "#     print(\"inside contents:\" +str(soup.find('div', {\"id\": \"article_text\"}).find(\"a\").text))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "#   cpu코어갯수의 2배  ex: 4코어 i5는 8개, 4코어8스레드인 i7은 16개)\n",
    "    pool = Pool(8)\n",
    "    pool.map(get_single_article, [x for x in list(spider().values())])\n",
    "#     list_v = spider()\n",
    "# 리스트 형식으로 변경\n",
    "#     print(type(list(spider().values())))\n",
    "# [x for x in list(spider().values())] iterator로 변경 하여 map의 인자로 하나씩 전달.\n",
    "# pool은 이를 병렬처리하여 계산한다.\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ipynb파일 import하는법](http://nbviewer.jupyter.org/github/jupyter/notebook/blob/master/docs/source/examples/Notebook/Importing%20Notebooks.ipynb)\n",
    "\n",
    "[파이썬 멀티 프로세싱, Pool 사용법](https://m.blog.naver.com/PostView.nhn?blogId=townpharm&logNo=220951524843&proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5.5804948806762695 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# --- 5.5804948806762695 seconds ---\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# \n",
    "\n",
    "# 페이지네이션\n",
    "def spider(max_pages=1):\n",
    "    page = 0\n",
    "    content_num = 10\n",
    "    data = {}\n",
    "    while page < max_pages:\n",
    "#       우리학교 url은 offset 번호를 바꾸어주면 page가 바뀐다.\n",
    "        url = 'http://www.ajou.ac.kr/main/ajou/notice.jsp?mode=list&board_no=33&pager.offset='+str(page*content_num)\n",
    "        req = requests.get(url)\n",
    "        html = req.text \n",
    "#         html.parser vs lxml\n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "        \n",
    "        for row in soup.find_all(\"tr\")[1:]:\n",
    "            tmp = row.find(\"a\")\n",
    "#             a링크에 붙여진 link를 붙여준다.\n",
    "            href = \"http://www.ajou.ac.kr/main/ajou/notice.jsp\" + tmp['href'] \n",
    "            data[tmp.text] = href\n",
    "#             print(data)\n",
    "        page += 1\n",
    "    \n",
    "    return data\n",
    "\n",
    "        \n",
    "def get_single_article(item_url):\n",
    "    req = requests.get(item_url)\n",
    "    html = req.text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "#     print(\"Inside_contents:\" + str(soup.find('div', {\"id\": \"article_text\"})))\n",
    "#     print(\"inside contents:\" +str(soup.find('div', {\"id\": \"article_text\"}).find(\"a\").text))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    for link in list(spider().values()):\n",
    "        get_single_article(link)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

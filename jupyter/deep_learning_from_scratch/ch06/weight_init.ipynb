{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 가중치의 초깃값\n",
    "> `가중치 감소(Weight decay)` = 오버피팅을 억제해 범용 성능을 높이는 테크닉, 가중치 매개변수의 값이 작아지도록 학습하는 방법, 가중치 값을 작게 하여 오버피팅이 일어나지 않게 하는 것\n",
    "\n",
    "## 6.2.1 초깃값을 0으로 하면?\n",
    "\n",
    "**\" 안된다 \"**\n",
    "왜냐하면 가중치를 균일한 값으로 설정해서는 안된다. 오차역전파법에서 모든 가중치의 값이 똑같이 갱신되기 때문에. 가중치가 고르게 되어버리는 상황을 막으려면( 가중치의 대칭적인 구조 ) **초깃값을 무작위로 설정해야함.**\n",
    "\n",
    "\n",
    "`기울기 손실(gradient vanishing)` = 데이터가 0과 1에 치우쳐 분포하게 되면 역전파의 기울기 값이 점점 작아지다가 사라진다. \n",
    "\n",
    "반데로 활성화 값들이 특정 한 곳에 치우쳐 있게되면, 뉴런을 여러개 둔 의미가 없어지게 된다. 즉 **표현력을 제한**하게 된다. \n",
    "\n",
    "> 각 층의 활성화값을 적당히 고루 분포되어야 한다. 층과 층 사이에 적당하게 다양한 데이터가 흐르게 해야 신경망 학습이 효율적으로 이뤄지기 때문이다. 반대로 치우친 데이터가 흐르면 기울기 소실이나 표현력 제한 문제에 빠져서 학습이 잘 이뤄지지 않는다.\n",
    "\n",
    "`Xavier 초기값` : 권장하는 가중치 초기값, 각층의 활성화값들을 광범위하게 분포시키려면 앞 계층의 노드가 n개라면 초깃값의 표준편차가 $ \\frac{1}{\\sqrt{n}} $이 되도록 설정  \n",
    "\n",
    "층이 깊어 지면서 형태가 일그러지는 것. (sigmoid는 (x,y) = (0,0.5)에서 대칭인 s곡선이기에 일그러지게 결과를 만드는데, tanh를 사용하게 되면 (0,0)대칭 s곡선이여서 말끔한 종모양으로 분포를 만들 수 있다. \n",
    "\n",
    "### Relu 사용할 때의 가중치 초기값\n",
    "\n",
    "Xavier 초깃값은 활성화 함수가 **선형**인 것을 전제로 이끈 결과물이다.(sigmoid나 tanh함수는 좌우대칭이라 중앙 부근이 선형인 함수로 볼 수 있다.)\n",
    "\n",
    "`He 초깃값`: 노드가 n개라면 초깃값의 표준편차가 $ \\sqrt{\\frac{2}{n}} $이 되도록 설정, ReLU는 음의 영역이 0이라서 더 넓게 분포시키기 위해 2배의 계수가 필요하다고 간단히 해석가능\n",
    "\n",
    "\n",
    "\n",
    "**결론: 활성화 함수가 s자 모양 곡선(sigmoid, tanh)일때는 Xavier초깃값, ReLU일때는 He초깃값 **\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

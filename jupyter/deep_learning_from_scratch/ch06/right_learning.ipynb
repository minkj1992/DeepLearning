{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 바른 학습\n",
    "\n",
    "## 6.4.1 오버피팅\n",
    "\n",
    "+ 일어나는 원인\n",
    "    + 매개변수가 많고 표현력이 높은 모델\n",
    "    + 훈련 데이터가 적을때\n",
    "    \n",
    "+ 해결법( 오버피팅을 억제하는 정규화 기술)\n",
    "    + 가중치 감소(weight decay)\n",
    "    + 드롭아웃  \n",
    "\n",
    "## 6.4.2 가중치 감소\n",
    "> 학습 과정에서 큰 가중치에 대해서 그에 상응하는 큰 페널티를 부과하여, 오버피팅을 억제하는 방법( 오버 피팅의 대다수가 가중치 매개변수의 값이 커서 발생하는 경우가 많다)\n",
    "\n",
    "+ 가중치 제곱 법칙(L2 법칙)\n",
    "W -> L2 법칙 -> 가중치 감소 $ \\frac{1*\\lambda*W^2}{2} $\n",
    "\n",
    "여기서 람다는 정규화의 세기를 조절하는 하이퍼파라미터, 람다가 클수록 큰 가중치에 대한 페널티가 커진다. \n",
    "1/2 은 $\\lambda*W^2$의 미분값이 2를 조정하는 역할의 상수이다\n",
    "\n",
    "가중치 감소는 모든 가중치 각각의 손실함수에 $ \\frac{1*\\lambda*W^2}{2} $ 을 더한다. 따라서 가중치의 기울기를 구하는 계산에서는 그동안의 오차역전파법에 따른 결과에 정규화 항을 미분한 $ \\lambda*W $을 더한다. (미분을 시키기 때문에)\n",
    "\n",
    "+ L1 법칙은 각각을 절대값의 합\n",
    "+ L infinite 법칙은 Max 법칙으로, 각 원소의 절댓값 중 가장 큰 것에 해당한다.\n",
    "\n",
    "가중치 감소를 하게 되면, test와 train 사이의 차이를 줄일 수 있다. 또한 가중치 감소를 시행하기 전과 달리 train 정확도가 100%에 도달하지 못하는 현상도 일어난다. (이후 설명)\n",
    "\n",
    "## 6.4.3 드롭아웃\n",
    "\n",
    "신경망이 복잡해지면 가중치 감소만으로는 대응하기 어려워진다.\n",
    "\n",
    "> 드롭아웃은 뉴런을 임의로 삭제하면서 학습하는 방법. 훈련 때 hidden layer의 뉴런을 무작위로 골라 삭제한다.(뉴런에 신호를 전달하지 못하도록 한다는 뜻), test때는 모든 뉴런에 신호를 전달하도록 한다. 단 test때는 각 뉴런의 출력에 훈련 때 삭제한 비율을 곱하여 출력해준다. \n",
    "\n",
    "\n",
    "c.f 앙상블 학습(여러 샘플 시험들을 평균내어 정확도에 답을 내는 방식)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

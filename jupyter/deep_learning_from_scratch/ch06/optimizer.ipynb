{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization 1번째 방법 (확률적 경사하강법)\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "#         lr = learning rate\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "#         params = params['W1'], grads['W1']처럼 딕셔너리 변수로, 각각 가중치 매개변수와 기울기를 저장하고 있다.\n",
    "        for key in params.key():\n",
    "#             기울기의 방향으로 lr만큼 움직인다.\n",
    "            params[key] -= self.lr * grads[key]\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD\n",
    "\n",
    "> 기울어진 방향으로 lr만큼 움직인다.\n",
    "\n",
    "+ 장점: 구현이 쉽다.\n",
    "+ 단점: 비등방성함수(anisotropy, 기울기가 달라지는 함수) 지그재그로 움직인다. 비효율적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2번째 모멘텀\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        slef.v = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if sele.v in None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
    "            params[key] += self.v[key]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum\n",
    "\n",
    "+ 기울기 방향으로 힘을 받아 물체가 가속된다는 물리법칙을 적용한 optimizer\n",
    "\n",
    "+ v: velocity(속도)\n",
    "+ $ \\alpha v $ = 물체가 아무런 힘을 받지 않을 때 서서히 descent 역할, 물리에서의 지면 마찰이나 공기저항에 해당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad -> RMSProp(optimized version)\n",
    "\n",
    "> `학습률 감소(learning rate deay)` = lr을 구하는 기술, 학습을 진행하면서 학습률을 점차 줄여가는 방법. 처음에는 크게 학습하다가 조금씩 작게 학습하는 방법.\n",
    "\n",
    "각각의 매개변수에 맞춤형으로 학습률을값을 만들어준다.\n",
    "\n",
    "+ h = 단계를 거듭할수록 커진다. W (각각의 W 즉 매개변수에 따라서 값이 달라진다.)\n",
    "\n",
    "$W = W - lr * \\frac{1}{\\sqrt{h}} \\frac{\\partial L}{\\partial W} $\n",
    "\n",
    "매개변수의 원소 중에서 많이 움직인(크게 갱신된)원소는 학습률이 낮아진다는 뜻, 다시 말해 학습률 감소가 매개변수의 원소마다 다르게 적용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam\n",
    "\n",
    "> Momentum + AdaGrad 융합한듯한 방법\n",
    "\n",
    "+ 매개변수 공간을 효율적으로 탐색\n",
    "+ 하이퍼파라미터의 'bias 보정'이 진행된다.\n",
    "+ 학습의 갱신 강도를 적응적으로 조정함으로써, 모멘텀 때보다 공의 좌우 흔들림이 적다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
